<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>12.2. Virtualization</title><meta name="generator" content="publican v4.3.2"/><meta name="keywords" content="RAID, LVM, FAI, Preseeding, Monitoring, Virtualization, Xen, LXC"/><link rel="prev" href="advanced-administration.html" title="Chương 12. Advanced Administration"/><link rel="next" href="sect.automated-installation.html" title="12.3. Automated Installation"/><link xmlns="" rel="canonical" href="https://debian-handbook.info/browse/vi-VN/stable/sect.virtualization.html"/></head><body onLoad="initSwitchery(); jQuery(&quot;#poptoc&quot;).load('index.html .toc:eq(0)'); jQuery('.programlisting').each(function(i, block){hljs.highlightBlock(block);});" onClick="hide('poptoc');"><header><div id="banner"><a href="http://debian-handbook.info/get/"><span class="text">Download the ebook</span></a></div><ul class="docnav top"><li class="previous"><a accesskey="p" href="advanced-administration.html"><strong>Trước đó</strong></a></li><li class="home" onClick="work=1;showhide('poptoc');">Sổ tay Quản trị Debian</li><li class="next"><a accesskey="n" href="sect.automated-installation.html"><strong>Kế tiếp</strong></a></li></ul></header><div id="poptoc" class="hidden"> </div><section class="section" id="sect.virtualization"><div class="titlepage"><div><div><h2 class="title"><a xmlns="" id="sect.virtualization"/>12.2. Virtualization</h2></div></div></div>
		
		 <a id="id-1.15.5.2" class="indexterm"></a>
		 <p>
			Virtualization is one of the most major advances in the recent years of computing. The term covers various abstractions and techniques simulating virtual computers with a variable degree of independence on the actual hardware. One physical server can then host several systems working at the same time and in isolation. Applications are many, and often derive from this isolation: test environments with varying configurations for instance, or separation of hosted services across different virtual machines for security.
		</p>
		 <p>
			There are multiple virtualization solutions, each with its own pros and cons. This book will focus on Xen, LXC, and KVM, but other noteworthy implementations include the following:
		</p>
		 <a id="id-1.15.5.5" class="indexterm"></a>
		 <a id="id-1.15.5.6" class="indexterm"></a>
		 <a id="id-1.15.5.7" class="indexterm"></a>
		 <a id="id-1.15.5.8" class="indexterm"></a>
		 <a id="id-1.15.5.9" class="indexterm"></a>
		 <a id="id-1.15.5.10" class="indexterm"></a>
		 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
				<p>
					QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <span class="emphasis"><em>amd64</em></span> system can emulate an <span class="emphasis"><em>arm</em></span> computer. QEMU is free software. <a class="ulink" href="http://www.qemu.org/">http://www.qemu.org/</a>
				</p>

			</li><li class="listitem">
				<p>
					Bochs is another free virtual machine, but it only emulates the x86 architectures (i386 and amd64).
				</p>

			</li><li class="listitem">
				<p>
					VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <a class="ulink" href="http://www.vmware.com/">http://www.vmware.com/</a>
				</p>

			</li><li class="listitem">
				<p>
					VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler. While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <a class="ulink" href="http://www.virtualbox.org/">http://www.virtualbox.org/</a>
				</p>

			</li></ul></div>
		 <section class="section" id="sect.xen"><div class="titlepage"><div><div><h3 class="title"><a xmlns="" id="sect.xen"/>12.2.1. Xen</h3></div></div></div>
			
			 <p>
				Xen <a id="id-1.15.5.12.2.1" class="indexterm"></a> is a “paravirtualization” solution. It introduces a thin abstraction layer, called a “hypervisor”, between the hardware and the upper systems; this acts as a referee that controls access to hardware from the virtual machines. However, it only handles a few of the instructions, the rest is directly executed by the hardware on behalf of the systems. The main advantage is that performances are not degraded, and systems run close to native speed; the drawback is that the kernels of the operating systems one wishes to use on a Xen hypervisor need to be adapted to run on Xen.
			</p>
			 <p>
				Let's spend some time on terms. The hypervisor is the lowest layer, that runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <span class="emphasis"><em>domains</em></span>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <span class="emphasis"><em>dom0</em></span>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <span class="emphasis"><em>domU</em></span>. In other words, and from a user point of view, the <span class="emphasis"><em>dom0</em></span> matches the “host” of other virtualization systems, while a <span class="emphasis"><em>domU</em></span> can be seen as a “guest”.
			</p>
			 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>CULTURE</em></span> Xen and the various versions of Linux</strong></p></div></div></div> 
			 <p>
				Xen was initially developed as a set of patches that lived out of the official tree, and not integrated to the Linux kernel. At the same time, several upcoming virtualization systems (including KVM) required some generic virtualization-related functions to facilitate their integration, and the Linux kernel gained this set of functions (known as the <span class="emphasis"><em>paravirt_ops</em></span> or <span class="emphasis"><em>pv_ops</em></span> interface). Since the Xen patches were duplicating some of the functionality of this interface, they couldn't be accepted officially.
			</p>
			 <p>
				Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <a class="ulink" href="http://wiki.xenproject.org/wiki/XenParavirtOps">http://wiki.xenproject.org/wiki/XenParavirtOps</a>
			</p>
			 <p>
				Since <span class="distribution distribution">Jessie</span> is based on version 3.16 of the Linux kernel, the standard <span class="pkg pkg">linux-image-686-pae</span> and <span class="pkg pkg">linux-image-amd64</span> packages include the necessary code, and the distribution-specific patching that was required for <span class="distribution distribution">Squeeze</span> and earlier versions of Debian is no more. <a class="ulink" href="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix">http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix</a>
			</p>
			 </div> <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>NOTE</em></span> Architectures compatible with Xen</strong></p></div></div></div> 
			 <p>
				Xen is currently only available for the i386, amd64, arm64 and armhf architectures.
			</p>
			 </div> <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>CULTURE</em></span> Xen and non-Linux kernels</strong></p></div></div></div> 
			 <p>
				Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <a class="ulink" href="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen">http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen</a> <a class="ulink" href="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen">http://wiki.xenproject.org/wiki/DomU_Support_for_Xen</a>
			</p>
			 <p>
				However, if Xen can rely on the hardware functions dedicated to virtualization (which are only present in more recent processors), even non-modified operating systems can run as domU (including Windows).
			</p>
			 </div> <p>
				Using Xen under Debian requires three components:
			</p>
			 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<p>
						The hypervisor itself. According to the available hardware, the appropriate package will be either <span class="pkg pkg">xen-hypervisor-4.4-amd64</span>, <span class="pkg pkg">xen-hypervisor-4.4-armhf</span>, or <span class="pkg pkg">xen-hypervisor-4.4-arm64</span>.
					</p>

				</li><li class="listitem">
					<p>
						A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 3.16 version present in <span class="distribution distribution">Jessie</span>.
					</p>

				</li><li class="listitem">
					<p>
						The i386 architecture also requires a standard library with the appropriate patches taking advantage of Xen; this is in the <span class="pkg pkg">libc6-xen</span> package.
					</p>

				</li></ul></div>
			 <p>
				In order to avoid the hassle of selecting these components by hand, a few convenience packages (such as <span class="pkg pkg">xen-linux-system-amd64</span>) have been made available; they all pull in a known-good combination of the appropriate hypervisor and kernel packages. The hypervisor also brings <span class="pkg pkg">xen-utils-4.4</span>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the Grub bootloader menu, so as to start the chosen kernel in a Xen dom0. Note however that this entry is not usually set to be the first one in the list, and will therefore not be selected by default. If that is not the desired behavior, the following commands will change it:
			</p>
			 
<pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen </code></strong><code class="computeroutput"># </code><strong class="userinput"><code>update-grub </code></strong></pre>
			 <p>
				Once these prerequisites are installed, the next step is to test the behavior of the dom0 by itself; this involves a reboot to the hypervisor and the Xen kernel. The system should boot in its standard fashion, with a few extra messages on the console during the early initialization steps.
			</p>
			 <p>
				Now is the time to actually install useful systems on the domU systems, using the tools from <span class="pkg pkg">xen-tools</span>. This package provides the <span class="command"><strong>xen-create-image</strong></span> command, which largely automates the task. The only mandatory parameter is <code class="literal">--hostname</code>, giving a name to the domU; other options are important, but they can be stored in the <code class="filename">/etc/xen-tools/xen-tools.conf</code> configuration file, and their absence from the command line doesn't trigger an error. It is therefore important to either check the contents of this file before creating images, or to use extra parameters in the <span class="command"><strong>xen-create-image</strong></span> invocation. Important parameters of note include the following:
			</p>
			 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<p>
						<code class="literal">--memory</code>, to specify the amount of RAM dedicated to the newly created system;
					</p>

				</li><li class="listitem">
					<p>
						<code class="literal">--size</code> and <code class="literal">--swap</code>, to define the size of the “virtual disks” available to the domU;
					</p>

				</li><li class="listitem">
					<p>
						<code class="literal">--debootstrap</code>, to cause the new system to be installed with <span class="command"><strong>debootstrap</strong></span>; in that case, the <code class="literal">--dist</code> option will also most often be used (with a distribution name such as <span class="distribution distribution">jessie</span>).
					</p>
					 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>GOING FURTHER</em></span> Installing a non-Debian system in a domU</strong></p></div></div></div> 
					 <p>
						In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <code class="literal">--kernel</code> option.
					</p>
					 </div>
				</li><li class="listitem">
					<p>
						<code class="literal">--dhcp</code> states that the domU's network configuration should be obtained by DHCP while <code class="literal">--ip</code> allows defining a static IP address.
					</p>

				</li><li class="listitem">
					<p>
						Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <code class="literal">--dir</code> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <code class="literal">--lvm</code> option, followed by the name of a volume group; <span class="command"><strong>xen-create-image</strong></span> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive.
					</p>
					 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>NOTE</em></span> Storage in the domU</strong></p></div></div></div> 
					 <p>
						Entire hard disks can also be exported to the domU, as well as partitions, RAID arrays or pre-existing LVM logical volumes. These operations are not automated by <span class="command"><strong>xen-create-image</strong></span>, however, so editing the Xen image's configuration file is in order after its initial creation with <span class="command"><strong>xen-create-image</strong></span>.
					</p>
					 </div>
				</li></ul></div>
			 <p>
				Once these choices are made, we can create the image for our future Xen domU:
			</p>
			 
<pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</code></strong>
<code class="computeroutput"> [...] General Information -------------------- Hostname : testxen Distribution : jessie Mirror : http://ftp.debian.org/debian/ Partitions : swap 128Mb (swap) / 2G (ext3) Image type : sparse Memory size : 128Mb Kernel path : /boot/vmlinuz-3.16.0-4-amd64 Initrd path : /boot/initrd.img-3.16.0-4-amd64 [...] Logfile produced at: /var/log/xen-tools/testxen.log Installation Summary --------------------- Hostname : testxen Distribution : jessie MAC Address : 00:16:3E:8E:67:5C IP-Address(es) : dynamic RSA Fingerprint : 0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b Root Password : adaX2jyRHNuWm8BDJS7PcEJ </code></pre>
			 <p>
				We now have a virtual machine, but it is currently not running (and therefore only using space on the dom0's hard disk). Of course, we can create more images, possibly with different parameters.
			</p>
			 <p>
				Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces, that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:
			</p>
			 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<p>
						The simplest model is the <span class="emphasis"><em>bridge</em></span> model; all the eth0 network cards (both in the dom0 and the domU systems) behave as if they were directly plugged into an Ethernet switch.
					</p>

				</li><li class="listitem">
					<p>
						Then comes the <span class="emphasis"><em>routing</em></span> model, where the dom0 behaves as a router that stands between the domU systems and the (physical) external network.
					</p>

				</li><li class="listitem">
					<p>
						Finally, in the <span class="emphasis"><em>NAT</em></span> model, the dom0 is again between the domU systems and the rest of the network, but the domU systems are not directly accessible from outside, and traffic goes through some network address translation on the dom0.
					</p>

				</li></ul></div>
			 <p>
				These three networking nodes involve a number of interfaces with unusual names, such as <code class="filename">vif*</code>, <code class="filename">veth*</code>, <code class="filename">peth*</code> and <code class="filename">xenbr0</code>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model.
			</p>
			 <p>
				The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <span class="command"><strong>xend</strong></span> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <code class="filename">xenbr0</code> taking precedence if several such bridges exist). We must therefore set up a bridge in <code class="filename">/etc/network/interfaces</code> (which requires installing the <span class="pkg pkg">bridge-utils</span> package, which is why the <span class="pkg pkg">xen-utils-4.4</span> package recommends it) to replace the existing eth0 entry:
			</p>
			 
<pre class="programlisting">auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</pre>
			 <p>
				After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <span class="command"><strong>xl</strong></span> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them.
			</p>
			 
<pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>xl list</code></strong>
<code class="computeroutput">Name ID Mem VCPUs State Time(s) Domain-0 0 463 1 r----- 9.8 # </code><strong class="userinput"><code>xl create /etc/xen/testxen.cfg</code></strong>
<code class="computeroutput">Parsing config from /etc/xen/testxen.cfg # </code><strong class="userinput"><code>xl list</code></strong>
<code class="computeroutput">Name ID Mem VCPUs State Time(s) Domain-0 0 366 1 r----- 11.4 testxen 1 128 1 -b---- 1.1</code></pre>
			 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>TOOL</em></span> Choice of toolstacks to manage Xen VM</strong></p></div></div></div> 
			 <a id="id-1.15.5.12.24.2" class="indexterm"></a>
			 <a id="id-1.15.5.12.24.3" class="indexterm"></a>
			 <p>
				In Debian 7 and older releases, <span class="command"><strong>xm</strong></span> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <span class="command"><strong>xl</strong></span> which is mostly backwards compatible. But those are not the only available tools: <span class="command"><strong>virsh</strong></span> of libvirt and <span class="command"><strong>xe</strong></span> of XenServer's XAPI (commercial offering of Xen) are alternative tools.
			</p>
			 </div> <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>CAUTION</em></span> Only one domU per image!</strong></p></div></div></div> 
			 <p>
				While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is however quite possible to reuse a single swap partition, or the partition hosting the <code class="filename">/home</code> filesystem.
			</p>
			 </div> <p>
				Note that the <code class="filename">testxen</code> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly.
			</p>
			 <p>
				Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <code class="filename">hvc0</code> console, with the <span class="command"><strong>xl console</strong></span> command:
			</p>
			 
<pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>xl console testxen</code></strong>
<code class="computeroutput">[...] Debian GNU/Linux 8 testxen hvc0 testxen login: </code></pre>
			 <p>
				One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <span class="keycap"><strong>Control</strong></span>+<span class="keycap"><strong>]</strong></span> key combination.
			</p>
			 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>TIP</em></span> Getting the console straight away</strong></p></div></div></div> 
			 <p>
				Sometimes one wishes to start a domU system and get to its console straight away; this is why the <span class="command"><strong>xl create</strong></span> command takes a <code class="literal">-c</code> switch. Starting a domU with this switch will display all the messages as the system boots.
			</p>
			 </div> <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>TOOL</em></span> OpenXenManager</strong></p></div></div></div> 
			 <p>
				OpenXenManager (in the <span class="pkg pkg">openxenmanager</span> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <span class="command"><strong>xl</strong></span> command.
			</p>
			 </div> <p>
				Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <span class="command"><strong>xl pause</strong></span> and <span class="command"><strong>xl unpause</strong></span> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <span class="command"><strong>xl save</strong></span> and <span class="command"><strong>xl restore</strong></span> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system.
			</p>
			 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>DOCUMENTATION</em></span> <span class="command"><strong>xl</strong></span> options</strong></p></div></div></div> 
			 <p>
				Most of the <span class="command"><strong>xl</strong></span> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <span class="citerefentry"><span class="refentrytitle">xl</span>
				 (1)</span> manual page.
			</p>
			 </div> <p>
				Halting or rebooting a domU can be done either from within the domU (with the <span class="command"><strong>shutdown</strong></span> command) or from the dom0, with <span class="command"><strong>xl shutdown</strong></span> or <span class="command"><strong>xl reboot</strong></span>.
			</p>
			 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>GOING FURTHER</em></span> Advanced Xen</strong></p></div></div></div> 
			 <p>
				Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <a class="ulink" href="http://www.xen.org/support/documentation.html">http://www.xen.org/support/documentation.html</a>
			</p>
			 </div>
		</section>
		 <section class="section" id="sect.lxc"><div class="titlepage"><div><div><h3 class="title"><a xmlns="" id="sect.lxc"/>12.2.2. LXC</h3></div></div></div>
			
			 <a id="id-1.15.5.13.2" class="indexterm"></a>
			 <p>
				Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <span class="emphasis"><em>control groups</em></span>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system.
			</p>
			 <p>
				These features can be combined to isolate a whole process family starting from the <span class="command"><strong>init</strong></span> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <span class="emphasis"><em>LinuX Containers</em></span>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether).
			</p>
			 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>NOTE</em></span> LXC isolation limits</strong></p></div></div></div> 
			 <p>
				LXC containers do not provide the level of isolation achieved by heavier emulators or virtualizers. In particular:
			</p>
			 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<p>
						since the kernel is shared among the host system and the containers, processes constrained to containers can still access the kernel messages, which can lead to information leaks if messages are emitted by a container;
					</p>

				</li><li class="listitem">
					<p>
						for similar reasons, if a container is compromised and a kernel vulnerability is exploited, the other containers may be affected too;
					</p>

				</li><li class="listitem">
					<p>
						on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers.
					</p>

				</li></ul></div>
			 </div> <p>
				Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container.
			</p>
			 <section class="section" id="id-1.15.5.13.7"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="id-1.15.5.13.7"/>12.2.2.1. Preliminary Steps</h4></div></div></div>
				
				 <p>
					The <span class="pkg pkg">lxc</span> package contains the tools required to run LXC, and must therefore be installed.
				</p>
				 <p>
					LXC also requires the <span class="emphasis"><em>control groups</em></span> configuration system, which is a virtual filesystem to be mounted on <code class="filename">/sys/fs/cgroup</code>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration.
				</p>

			</section>
			 <section class="section" id="sect.lxc.network"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="sect.lxc.network"/>12.2.2.2. Network Configuration</h4></div></div></div>
				
				 <p>
					The goal of installing LXC is to set up virtual machines; while we could of course keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <span class="pkg pkg">bridge-utils</span> package will be required.
				</p>
				 <p>
					The simple case is just a matter of editing <code class="filename">/etc/network/interfaces</code>, moving the configuration for the physical interface (for instance <code class="literal">eth0</code>) to a bridge interface (usually <code class="literal">br0</code>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:
				</p>
				 
<pre class="programlisting">auto eth0
iface eth0 inet dhcp</pre>
				 <p>
					They should be disabled and replaced with the following:
				</p>
				 
<pre class="programlisting">#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</pre>
				 <p>
					The effect of this configuration will be similar to what would be obtained if the containers were machines plugged into the same physical network as the host. The “bridge” configuration manages the transit of Ethernet frames between all the bridged interfaces, which includes the physical <code class="literal">eth0</code> as well as the interfaces defined for the containers.
				</p>
				 <p>
					In cases where this configuration cannot be used (for instance if no public IP addresses can be assigned to the containers), a virtual <span class="emphasis"><em>tap</em></span> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world.
				</p>
				 <p>
					In addition to <span class="pkg pkg">bridge-utils</span>, this “rich” configuration requires the <span class="pkg pkg">vde2</span> package; the <code class="filename">/etc/network/interfaces</code> file then becomes:
				</p>
				 
<pre class="programlisting"># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0
</pre>
				 <p>
					The network can then be set up either statically in the containers, or dynamically with DHCP server running on the host. Such a DHCP server will need to be configured to answer queries on the <code class="literal">br0</code> interface.
				</p>

			</section>
			 <section class="section" id="id-1.15.5.13.9"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="id-1.15.5.13.9"/>12.2.2.3. Setting Up the System</h4></div></div></div>
				
				 <p>
					Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <span class="pkg pkg">lxc</span> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <span class="pkg pkg">debootstrap</span> and <span class="pkg pkg">rsync</span> packages) will install a Debian container:
				</p>
				 
<pre class="screen"><code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>lxc-create -n testlxc -t debian </code></strong><code class="computeroutput">debootstrap is /usr/sbin/debootstrap Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... Downloading debian minimal ... I: Retrieving Release I: Retrieving Release.gpg [...] Download complete. Copying rootfs to /var/lib/lxc/testlxc/rootfs... [...] Root password is 'sSiKhMzI', please change ! root@mirwiz:~# </code>
</pre>
				 <p>
					Note that the filesystem is initially created in <code class="filename">/var/cache/lxc</code>, then moved to its destination directory. This allows creating identical containers much more quickly, since only copying is then required.
				</p>
				 <p>
					Note that the debian template creation script accepts an <code class="option">--arch</code> option to specify the architecture of the system to be installed and a <code class="option">--release</code> option if you want to install something else than the current stable release of Debian. You can also set the <code class="literal">MIRROR</code> environment variable to point to a local Debian mirror.
				</p>
				 <p>
					The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<code class="filename">/var/lib/lxc/testlxc/config</code>) and add a few <code class="literal">lxc.network.*</code> entries:
				</p>
				 
<pre class="programlisting">lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20
</pre>
				 <p>
					These entries mean, respectively, that a virtual interface will be created in the container; that it will automatically be brought up when said container is started; that it will automatically be connected to the <code class="literal">br0</code> bridge on the host; and that its MAC address will be as specified. Should this last entry be missing or disabled, a random MAC address will be generated.
				</p>
				 <p>
					Another useful entry in that file is the setting of the hostname:
				</p>
				 
<pre class="programlisting">lxc.utsname = testlxc
</pre>

			</section>
			 <section class="section" id="id-1.15.5.13.10"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="id-1.15.5.13.10"/>12.2.2.4. Starting the Container</h4></div></div></div>
				
				 <p>
					Now that our virtual machine image is ready, let's start the container:
				</p>
				 
<pre class="screen scale scale" width="94"><code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>lxc-start --daemon --name=testlxc </code></strong><code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>lxc-console -n testlxc </code></strong><code class="computeroutput">Debian GNU/Linux 8 testlxc tty1 testlxc login: </code><strong class="userinput"><code>root</code></strong><code class="computeroutput"> Password: Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. root@testlxc:~# </code><strong class="userinput"><code>ps auxwf</code></strong>
<code class="computeroutput">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.2 28164 4432 ? Ss 17:33 0:00 /sbin/init root 20 0.0 0.1 32960 3160 ? Ss 17:33 0:00 /lib/systemd/systemd-journald root 82 0.0 0.3 55164 5456 ? Ss 17:34 0:00 /usr/sbin/sshd -D root 87 0.0 0.1 12656 1924 tty2 Ss+ 17:34 0:00 /sbin/agetty --noclear tty2 linux root 88 0.0 0.1 12656 1764 tty3 Ss+ 17:34 0:00 /sbin/agetty --noclear tty3 linux root 89 0.0 0.1 12656 1908 tty4 Ss+ 17:34 0:00 /sbin/agetty --noclear tty4 linux root 90 0.0 0.1 63300 2944 tty1 Ss 17:34 0:00 /bin/login -- root 117 0.0 0.2 21828 3668 tty1 S 17:35 0:00 \_ -bash root 268 0.0 0.1 19088 2572 tty1 R+ 17:39 0:00 \_ ps auxfw root 91 0.0 0.1 14228 2356 console Ss+ 17:34 0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102 root 197 0.0 0.4 25384 7640 ? Ss 17:38 0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e root 266 0.0 0.1 12656 1840 ? Ss 17:39 0:00 /sbin/agetty --noclear tty5 linux root 267 0.0 0.1 12656 1928 ? Ss 17:39 0:00 /sbin/agetty --noclear tty6 linux root@testlxc:~# </code></pre>
				 <p>
					We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<code class="filename">/var/lib/lxc/testlxc/rootfs</code>). We can exit the console with <span class="keycap"><strong>Control</strong></span>+<span class="keycap"><strong>a</strong></span> <span class="keycap"><strong>q</strong></span>.
				</p>
				 <p>
					Note that we ran the container as a background process, thanks to the <code class="option">--daemon</code> option of <span class="command"><strong>lxc-start</strong></span>. We can interrupt the container with a command such as <span class="command"><strong>lxc-stop --name=testlxc</strong></span>.
				</p>
				 <p>
					The <span class="pkg pkg">lxc</span> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <span class="command"><strong>lxc-autostart</strong></span> which starts containers whose <code class="literal">lxc.start.auto</code> option is set to 1). Finer-grained control of the startup order is possible with <code class="literal">lxc.start.order</code> and <code class="literal">lxc.group</code>: by default, the initialization script first starts containers which are part of the <code class="literal">onboot</code> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <code class="literal">lxc.start.order</code> option.
				</p>
				 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>GOING FURTHER</em></span> Mass virtualization</strong></p></div></div></div> 
				 <p>
					Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <code class="literal">tap</code> and <code class="literal">veth</code> interfaces should be enough in many cases.
				</p>
				 <p>
					It may also make sense to share part of the filesystem, such as the <code class="filename">/usr</code> and <code class="filename">/lib</code> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <code class="literal">lxc.mount.entry</code> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage.
				</p>
				 <p>
					We haven't described all the available options, of course; more comprehensive information can be obtained from the <span class="citerefentry"> <span class="refentrytitle">lxc</span>
					 (7) </span> and <span class="citerefentry"> <span class="refentrytitle">lxc.container.conf</span>
					 (5)</span> manual pages and the ones they reference.
				</p>
				 </div>
			</section>

		</section>
		 <section class="section" id="id-1.15.5.14"><div class="titlepage"><div><div><h3 class="title"><a xmlns="" id="id-1.15.5.14"/>12.2.3. Virtualization with KVM</h3></div></div></div>
			
			 <a id="id-1.15.5.14.2" class="indexterm"></a>
			 <p>
				KVM, which stands for <span class="emphasis"><em>Kernel-based Virtual Machine</em></span>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <span class="command"><strong>qemu-*</strong></span> commands: it is still about KVM.
			</p>
			 <p>
				Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <code class="filename">/proc/cpuinfo</code>.
			</p>
			 <p>
				With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization.
			</p>
			 <section class="section" id="id-1.15.5.14.6"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="id-1.15.5.14.6"/>12.2.3.1. Preliminary Steps</h4></div></div></div>
				
				 <a id="id-1.15.5.14.6.2" class="indexterm"></a>
				 <p>
					Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The <span class="pkg pkg">qemu-kvm</span> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules.
				</p>
				 <a id="id-1.15.5.14.6.4" class="indexterm"></a>
				 <a id="id-1.15.5.14.6.5" class="indexterm"></a>
				 <p>
					Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <span class="emphasis"><em>libvirt</em></span> library and the associated <span class="emphasis"><em>virtual machine manager</em></span> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare and UML). <span class="command"><strong>virtual-manager</strong></span> is a graphical interface that uses libvirt to create and manage virtual machines.
				</p>
				 <a id="id-1.15.5.14.6.7" class="indexterm"></a>
				 <p>
					We first install the required packages, with <span class="command"><strong>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</strong></span>. <span class="pkg pkg">libvirt-bin</span> provides the <span class="command"><strong>libvirtd</strong></span> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. In addition, this package provides the <span class="command"><strong>virsh</strong></span> command-line tool, which allows controlling the <span class="command"><strong>libvirtd</strong></span>-managed machines.
				</p>
				 <p>
					The <span class="pkg pkg">virtinst</span> package provides <span class="command"><strong>virt-install</strong></span>, which allows creating virtual machines from the command line. Finally, <span class="pkg pkg">virt-viewer</span> allows accessing a VM's graphical console.
				</p>

			</section>
			 <section class="section" id="id-1.15.5.14.7"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="id-1.15.5.14.7"/>12.2.3.2. Network Configuration</h4></div></div></div>
				
				 <p>
					Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <a class="xref" href="sect.virtualization.html#sect.lxc.network" title="12.2.2.2. Network Configuration">Phần 12.2.2.2, “Network Configuration”</a>).
				</p>
				 <p>
					Alternatively, and in the default configuration provided by KVM, the virtual machine is assigned a private address (in the 192.168.122.0/24 range), and NAT is set up so that the VM can access the outside network.
				</p>
				 <p>
					The rest of this section assumes that the host has an <code class="literal">eth0</code> physical interface and a <code class="literal">br0</code> bridge, and that the former is connected to the latter.
				</p>

			</section>
			 <section class="section" id="id-1.15.5.14.8"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="id-1.15.5.14.8"/>12.2.3.3. Installation with <span class="command"><strong>virt-install</strong></span></h4></div></div></div>
				
				 <a id="id-1.15.5.14.8.2" class="indexterm"></a>
				 <p>
					Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line.
				</p>
				 <p>
					Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <a class="xref" href="sect.remote-login.html#sect.remote-desktops" title="9.2.2. Using Remote Graphical Desktops">Phần 9.2.2, “Using Remote Graphical Desktops”</a> for details), which will allow us to control the installation process.
				</p>
				 <p>
					We first need to tell libvirtd where to store the disk images, unless the default location (<code class="filename">/var/lib/libvirt/images/</code>) is fine.
				</p>
				 
<pre class="screen"><code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>mkdir /srv/kvm</code></strong>
<code class="computeroutput">root@mirwiz:~# </code><strong class="userinput"><code>virsh pool-create-as srv-kvm dir --target /srv/kvm</code></strong>
<code class="computeroutput">Pool srv-kvm created root@mirwiz:~# </code></pre>
				 <div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong><span class="emphasis"><em>TIP</em></span> Add your user to the libvirt group</strong></p></div></div></div> 
				 <p>
					All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <code class="literal">libvirt</code> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <code class="literal">libvirt</code> group and run the various commands under your user identity.
				</p>
				 </div> <p>
					Let us now start the installation process for the virtual machine, and have a closer look at <span class="command"><strong>virt-install</strong></span>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed.
				</p>
				 
<pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>virt-install --connect qemu:///system <span id="virtinst.connect"/><a xmlns="" id="virtinst.connect"/><span class="callout">1</span> --virt-type kvm <span id="virtinst.type"/><a xmlns="" id="virtinst.type"/><span class="callout">2</span> --name testkvm <span id="virtinst.name"/><a xmlns="" id="virtinst.name"/><span class="callout">3</span> --ram 1024 <span id="virtinst.ram"/><a xmlns="" id="virtinst.ram"/><span class="callout">4</span> --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <span id="virtinst.disk"/><a xmlns="" id="virtinst.disk"/><span class="callout">5</span> --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso <span id="virtinst.cdrom"/><a xmlns="" id="virtinst.cdrom"/><span class="callout">6</span> --network bridge=br0 <span id="virtinst.network"/><a xmlns="" id="virtinst.network"/><span class="callout">7</span> --vnc <span id="virtinst.vnc"/><a xmlns="" id="virtinst.vnc"/><span class="callout">8</span> --os-type linux <span id="virtinst.os"/><a xmlns="" id="virtinst.os"/><span class="callout">9</span> --os-variant debianwheezy </code></strong><code class="computeroutput"> Starting install... Allocating 'testkvm.qcow' | 10 GB 00:00 Creating domain... | 0 B 00:00 Guest installation complete... restarting guest. </code></pre>
				 <div class="calloutlist"><dl class="calloutlist"><dt><a href="#virtinst.connect"><span class="callout">1</span></a> </dt><dd>
						<p>
							The <code class="literal">--connect</code> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<code class="literal">xen://</code>, <code class="literal">qemu://</code>, <code class="literal">lxc://</code>, <code class="literal">openvz://</code>, <code class="literal">vbox://</code>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<code class="literal">/system</code>) from others (<code class="literal">/session</code>).
						</p>

					</dd><dt><a href="#virtinst.type"><span class="callout">2</span></a> </dt><dd>
						<p>
							Since KVM is managed the same way as QEMU, the <code class="literal">--virt-type kvm</code> allows specifying the use of KVM even though the URL looks like QEMU.
						</p>

					</dd><dt><a href="#virtinst.name"><span class="callout">3</span></a> </dt><dd>
						<p>
							The <code class="literal">--name</code> option defines a (unique) name for the virtual machine.
						</p>

					</dd><dt><a href="#virtinst.ram"><span class="callout">4</span></a> </dt><dd>
						<p>
							The <code class="literal">--ram</code> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine.
						</p>

					</dd><dt><a href="#virtinst.disk"><span class="callout">5</span></a> </dt><dd>
						<p>
							The <code class="literal">--disk</code> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <code class="literal">size</code> parameter. The <code class="literal">format</code> parameter allows choosing among several ways of storing the image file. The default format (<code class="literal">raw</code>) is a single file exactly matching the disk's size and contents. We picked a more advanced format here, that is specific to QEMU and allows starting with a small file that only grows when the virtual machine starts actually using space.
						</p>

					</dd><dt><a href="#virtinst.cdrom"><span class="callout">6</span></a> </dt><dd>
						<p>
							The <code class="literal">--cdrom</code> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <code class="literal">/dev/cdrom</code>).
						</p>

					</dd><dt><a href="#virtinst.network"><span class="callout">7</span></a> </dt><dd>
						<p>
							The <code class="literal">--network</code> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24).
						</p>

					</dd><dt><a href="#virtinst.vnc"><span class="callout">8</span></a> </dt><dd>
						<p>
							<code class="literal">--vnc</code> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <a class="xref" href="sect.remote-login.html#sect.ssh-port-forwarding" title="9.2.1.3. Creating Encrypted Tunnels with Port Forwarding">Phần 9.2.1.3, “Creating Encrypted Tunnels with Port Forwarding”</a>). Alternatively, the <code class="literal">--vnclisten=0.0.0.0</code> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly.
						</p>

					</dd><dt><a href="#virtinst.os"><span class="callout">9</span></a> </dt><dd>
						<p>
							The <code class="literal">--os-type</code> and <code class="literal">--os-variant</code> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there.
						</p>

					</dd></dl></div>
				 <p>
					At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <span class="command"><strong>virt-viewer</strong></span> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):
				</p>
				 
<pre class="screen"><code class="computeroutput">$ </code><strong class="userinput"><code>virt-viewer --connect qemu+ssh://root@<em class="replaceable">server</em>/system testkvm </code></strong><code class="computeroutput">root@server's password: root@server's password: </code></pre>
				 <p>
					When the installation process ends, the virtual machine is restarted, now ready for use.
				</p>

			</section>
			 <section class="section" id="id-1.15.5.14.9"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="id-1.15.5.14.9"/>12.2.3.4. Managing Machines with <span class="command"><strong>virsh</strong></span></h4></div></div></div>
				
				 <a id="id-1.15.5.14.9.2" class="indexterm"></a>
				 <p>
					Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <span class="command"><strong>libvirtd</strong></span> for the list of the virtual machines it manages:
				</p>
				 
<pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>virsh -c qemu:///system list --all Id Name State ---------------------------------- - testkvm shut off </code></strong></pre>
				 <p>
					Let's start our test virtual machine:
				</p>
				 
<pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>virsh -c qemu:///system start testkvm </code></strong><code class="computeroutput">Domain testkvm started</code></pre>
				 <p>
					We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <span class="command"><strong>vncviewer</strong></span>):
				</p>
				 
<pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>virsh -c qemu:///system vncdisplay testkvm </code></strong><code class="computeroutput">:0</code></pre>
				 <p>
					Other available <span class="command"><strong>virsh</strong></span> subcommands include:
				</p>
				 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<p>
							<code class="literal">reboot</code> to restart a virtual machine;
						</p>

					</li><li class="listitem">
						<p>
							<code class="literal">shutdown</code> to trigger a clean shutdown;
						</p>

					</li><li class="listitem">
						<p>
							<code class="literal">destroy</code>, to stop it brutally;
						</p>

					</li><li class="listitem">
						<p>
							<code class="literal">suspend</code> to pause it;
						</p>

					</li><li class="listitem">
						<p>
							<code class="literal">resume</code> to unpause it;
						</p>

					</li><li class="listitem">
						<p>
							<code class="literal">autostart</code> to enable (or disable, with the <code class="literal">--disable</code> option) starting the virtual machine automatically when the host starts;
						</p>

					</li><li class="listitem">
						<p>
							<code class="literal">undefine</code> to remove all traces of the virtual machine from <span class="command"><strong>libvirtd</strong></span>.
						</p>

					</li></ul></div>
				 <p>
					All these subcommands take a virtual machine identifier as a parameter.
				</p>

			</section>
			 <section class="section" id="id-1.15.5.14.10"><div class="titlepage"><div><div><h4 class="title"><a xmlns="" id="id-1.15.5.14.10"/>12.2.3.5. Installing an RPM based system in Debian with yum</h4></div></div></div>
				
				 <p>
					If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <span class="command"><strong>debootstrap</strong></span>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <span class="command"><strong>yum</strong></span> utility (available in the package of the same name).
				</p>
				 <p>
					The procedure requires using <span class="command"><strong>rpm</strong></span> to extract an initial set of files, including notably <span class="command"><strong>yum</strong></span> configuration files, and then calling <span class="command"><strong>yum</strong></span> to extract the remaining set of packages. But since we call <span class="command"><strong>yum</strong></span> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <code class="filename">/srv/centos</code>.
				</p>
				 
<pre class="screen"><code class="computeroutput"># </code><strong class="userinput"><code>rootdir="/srv/centos" </code></strong><code class="computeroutput"># </code><strong class="userinput"><code>mkdir -p "$rootdir" /etc/rpm </code></strong><code class="computeroutput"># </code><strong class="userinput"><code>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath </code></strong><code class="computeroutput"># </code><strong class="userinput"><code>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm </code></strong><code class="computeroutput"># </code><strong class="userinput"><code>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm </code></strong><code class="computeroutput">rpm: RPM should not be used directly install RPM packages, use Alien instead! rpm: However assuming you know what you are doing... warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY # </code><strong class="userinput"><code>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo </code></strong><code class="computeroutput"># </code><strong class="userinput"><code>yum --assumeyes --installroot $rootdir groupinstall core </code></strong><code class="computeroutput">[...] # </code><strong class="userinput"><code>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo </code></strong></pre>

			</section>

		</section>

	</section><footer/></body></html>